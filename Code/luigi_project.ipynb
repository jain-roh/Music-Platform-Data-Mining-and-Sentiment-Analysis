{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if Task6() is complete\n",
      "C:\\Users\\tawde\\Anaconda3\\lib\\site-packages\\luigi\\worker.py:401: UserWarning: Task Task6() without outputs has no custom complete() method\n",
      "  is_complete = task.complete()\n",
      "DEBUG: Checking if Task5() is complete\n",
      "INFO: Informed scheduler that task   Task6__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if Task4() is complete\n",
      "INFO: Informed scheduler that task   Task5__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if Task3() is complete\n",
      "INFO: Informed scheduler that task   Task4__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if Task2() is complete\n",
      "INFO: Informed scheduler that task   Task3__99914b932b   has status   PENDING\n",
      "DEBUG: Checking if Task1() is complete\n",
      "INFO: Informed scheduler that task   Task2__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   Task1__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 5\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) running   Task2()\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) done      Task2()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Task2__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 4\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) running   Task3()\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) done      Task3()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Task3__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 3\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) running   Task4()\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) done      Task4()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Task4__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 2\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) running   Task5()\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) done      Task5()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Task5__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) running   Task6()\n",
      "C:\\Users\\tawde\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "C:\\Users\\tawde\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:255: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "C:\\Users\\tawde\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:257: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "C:\\Users\\tawde\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:259: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "INFO: [pid 30616] Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) done      Task6()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Task6__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=417123465, workers=1, host=DESKTOP-62N23DK, username=tawde, pid=30616) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 6 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 Task1()\n",
      "* 5 ran successfully:\n",
      "    - 1 Task2()\n",
      "    - 1 Task3()\n",
      "    - 1 Task4()\n",
      "    - 1 Task5()\n",
      "    - 1 Task6()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import lxml\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn import model_selection, preprocessing,svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "import string\n",
    "import luigi\n",
    "from dropbox.files import WriteMode\n",
    "import dropbox\n",
    "dbx = dropbox.Dropbox('aDf1htlkmN8AAAAAAAAC-_IjjyFsEJBJU6fY86Y32dr1LH3kVthriApkX9h08kmf')\n",
    "os.remove(os.getcwd()+\"/wordcount_dataset.csv\")\n",
    "os.remove(os.getcwd()+\"/model_input.csv\")\n",
    "os.remove(os.getcwd()+\"/Vader_output.csv\")\n",
    "os.remove(os.getcwd()+\"/merged_data.csv\")\n",
    "class Task0(luigi.Task):\n",
    "    def run(self):\n",
    "        \n",
    "        dbx.files_download_to_file(os.getcwd()+'/Final_dataset.csv', '/DataScienceTeam9/'+ 'Final_dataset.csv')\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(os.getcwd()+\"/Final_dataset.csv\")\n",
    "class Task1(luigi.Task):\n",
    "    def requires(self):\n",
    "        yield Task0()\n",
    "    def run(self):\n",
    "        dbx.files_download_to_file(os.getcwd()+'/word_final.csv', '/DataScienceTeam9/'+ 'word_final.csv')\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(os.getcwd()+\"/word_final.csv\")\n",
    "class Task2(luigi.Task):\n",
    "    def requires(self):\n",
    "        yield Task1()\n",
    "    def run(self):\n",
    "        def generate_ngrams(s, n):\n",
    "            # Convert to lowercases\n",
    "            s = s.lower()\n",
    "    \n",
    "            # Replace all none alphanumeric characters with spaces\n",
    "            s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "            # Break sentence in the token, remove empty tokens\n",
    "            tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "            # Use the zip function to help us generate n-grams\n",
    "            # Concatentate the tokens into ngrams and return\n",
    "            unigrams = zip(*[tokens[i:] for i in range(1)])\n",
    "            bigrams = zip(*[tokens[i:] for i in range(2)])\n",
    "            #ngrams = unigram.append(bigram)\n",
    "            a = [\" \".join(unigram) for unigram in unigrams]\n",
    "            b = [\" \".join(bigram) for bigram in bigrams]\n",
    "            c = a+b\n",
    "            #print(a)\n",
    "            return c\n",
    "        dataset = []\n",
    "        df1 = pd.read_csv(Task0().output().path)\n",
    "        #df = pd.read_csv(\"Z:/ADS/a/textrank_final.csv\")\n",
    "        with open(Task0().output().path,encoding=\"utf8\", newline='') as myFile:\n",
    "            reader1 = csv.reader(myFile)\n",
    "            for row1 in reader1:\n",
    "                data = row1[2]\n",
    "        #print(data)\n",
    "                data = data.lower()\n",
    "                stop_words = set(stopwords.words(\"english\"))\n",
    "        #remove tags\n",
    "                data=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",data)\n",
    "        # remove special characters and digits\n",
    "                data=re.sub(\"(\\\\d|\\\\W)+\",\" \",data)\n",
    "        ##Convert to list from string\n",
    "                data = data.split()\n",
    "        #Lemmatisation\n",
    "                lem = WordNetLemmatizer()\n",
    "                data = [lem.lemmatize(word) for word in data if not word in stop_words]\n",
    "                data = \" \".join(data)\n",
    "                dataset.append(data)\n",
    "        x = 1\n",
    "        abc=[]\n",
    "        df = pd.read_csv(Task1().output().path,names=[\"0\",\"words\"])\n",
    "        with open(Task1().output().path, newline='') as myFile:\n",
    "            reader = csv.reader(myFile)\n",
    "            for row in reader:\n",
    "                abc.append(row[1])\n",
    "        mapping = {}\n",
    "        for text in dataset:\n",
    "            thisline = generate_ngrams(text, n=2)\n",
    "            for q in abc:\n",
    "        \n",
    "        #print(q)\n",
    "                for wd in thisline:\n",
    "            #print(wd)\n",
    "                    if q == wd:\n",
    "                        if wd in mapping:\n",
    "                            mapping[wd] +=1\n",
    "                 \n",
    "\n",
    "                    else:\n",
    "                        mapping.update({wd:1})\n",
    "                        \n",
    "\n",
    "\n",
    "            df[x]= df['words'].map(mapping)\n",
    "    #print(df)\n",
    "            x +=1\n",
    "            mapping.clear()\n",
    "        df2_transposed = df.transpose() # or df2.transpose()\n",
    "        df2_transposed = df2_transposed.replace(np.nan, 0)\n",
    "        df2_transposed = df2_transposed.drop(\"0\", axis=0)\n",
    "        df2_transposed = df2_transposed.drop(columns=0)\n",
    "        #df2_transposed.to_csv(\"Z:/ADS/project/pipeline/wordcount_dataset.csv\",index=False,header=False, encoding='utf8')\n",
    "        df2_transposed.to_csv(self.output().path,index=False,header=False, encoding='utf8')\n",
    "        #print('Task1 Completed')\n",
    "        \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(os.getcwd()+\"/wordcount_dataset.csv\")\n",
    "class Task3(luigi.Task):\n",
    "    def requires(self):\n",
    "        yield Task2()\n",
    "    def run(self):\n",
    "        #df = pd.read_csv(self.input())\n",
    "        \n",
    "        dataset = pd.read_csv(Task2().output().path)\n",
    "        df = dataset.copy()\n",
    "        df['Good Cost']=df.iloc[:,0:28].astype(bool).sum(axis=1)\n",
    "        df['Bad Cost']=df.iloc[:,28:46].astype(bool).sum(axis=1)\n",
    "        df['Good Recommendation']=df.iloc[:,46:73].astype(bool).sum(axis=1)\n",
    "        df['Bad Recommendation']=df.iloc[:,73:99].astype(bool).sum(axis=1)\n",
    "        df['Good Stability']=df.iloc[:,99:121].astype(bool).sum(axis=1)\n",
    "        df['Bad Stability']=df.iloc[:,121:148].astype(bool).sum(axis=1)\n",
    "        df['Ads']=df.iloc[:,149:172].astype(bool).sum(axis=1)\n",
    "        df['No Ads']=df.iloc[:,172:190].astype(bool).sum(axis=1)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1 = df[['Good Cost','Bad Cost','Good Recommendation','Bad Recommendation',\n",
    "                  'Good Stability','Bad Stability','Ads','No Ads']]\n",
    "        data = pd.read_csv(Task0().output().path,header=0,encoding='utf-8')\n",
    "        df1 = df1.drop([0],axis = 0)\n",
    "        df2 = data['App']\n",
    "        df2.index += 1\n",
    "        result = pd.concat([df1, df2], axis=1)\n",
    "        result = result.replace({'Spotify': '1'})\n",
    "        result = result.replace({'Pandora':'2'})\n",
    "        result = result.replace({'AppleMusic':'3'})\n",
    "        result = result.replace({'AmazonMusic':'4'})\n",
    "        #result.to_csv('z:/ADS/project/pipeline/model_input.csv',index = None)\n",
    "        result.to_csv(self.output().path,index=None)\n",
    "        #print('Task2 Completed')\n",
    "        with open(Task2().output().path, 'rb') as csv:\n",
    "            dbx.files_upload(csv.read(),'/DataScienceTeam9/wordcount_dataset.csv', mode=dropbox.files.WriteMode(\"overwrite\"))\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(os.getcwd()+\"/model_input.csv\")\n",
    "class Task4(luigi.Task):\n",
    "    def requires(self):\n",
    "        yield Task3()\n",
    "    def run(self):\n",
    "        \n",
    "        dataset = pd.read_csv(Task0().output().path,header=0,encoding='utf-8')\n",
    "        df = dataset.copy()\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        scores = []\n",
    "        for items in df['Review']:\n",
    "            score = analyser.polarity_scores(items)\n",
    "            scores.append(score)\n",
    "        final = []\n",
    "        for items in scores:\n",
    "            if items['compound'] >= 0.05:\n",
    "                final.append(\"Positive\")\n",
    "            elif items['compound'] <= -0.05:\n",
    "                final.append(\"Negative\")\n",
    "            else:\n",
    "                final.append(\"Neutral\")\n",
    "\n",
    "        final = pd.DataFrame(final,columns = ['Vader Sentiment'])\n",
    "        df['Vader Sentiment'] = final\n",
    "        df.to_csv(self.output().path,index=None)\n",
    "        with open(Task3().output().path, 'rb') as csv:\n",
    "            dbx.files_upload(csv.read(),'/DataScienceTeam9/model_input.csv', mode=dropbox.files.WriteMode(\"overwrite\"))\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(os.getcwd()+\"/Vader_output.csv\")\n",
    "class Task5(luigi.Task):\n",
    "    def requires(self):\n",
    "        yield Task4()\n",
    "        #yield Task2()\n",
    "    def run(self):\n",
    "        \n",
    "        df = pd.read_csv(Task4().output().path)\n",
    "        df1 = pd.read_csv(Task3().output().path)\n",
    "        df1 = df1[['Good Cost','Bad Cost','Good Recommendation','Bad Recommendation',\n",
    "                  'Good Stability','Bad Stability','Ads','No Ads']]\n",
    "        result = pd.concat([df, df1], axis=1)\n",
    "        result.to_csv(self.output().path,index=None)\n",
    "        with open(Task4().output().path, 'rb') as csv:\n",
    "            dbx.files_upload(csv.read(),'/DataScienceTeam9/Vader_output.csv', mode=dropbox.files.WriteMode(\"overwrite\"))\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(os.getcwd()+\"/merged_data.csv\")\n",
    "        \n",
    "class Task6(luigi.Task):\n",
    "    def requires(self):\n",
    "        yield Task5()\n",
    "        \n",
    "    def run(self):\n",
    "       \n",
    "        features = pd.read_csv(Task3().output().path)\n",
    "        df = features.copy()\n",
    "        df.loc[(df['Good Cost']>0),'Good Cost']=1\n",
    "        df.loc[(df['Good Cost'].isnull()),'Good Cost']=0\n",
    "        df.loc[(df['Bad Cost']>0),'Bad Cost']=1\n",
    "        df.loc[(df['Bad Cost'].isnull()),'Bad Cost']=0\n",
    "        df.loc[(df['Good Recommendation']>0),'Good Recommendation']=1\n",
    "        df.loc[(df['Good Recommendation'].isnull()),'Good Recommendation']=0\n",
    "        df.loc[(df['Bad Recommendation']>0),'Bad Recommendation']=1\n",
    "        df.loc[(df['Bad Recommendation'].isnull()),'Bad Recommendation']=0\n",
    "        df.loc[(df['Good Stability']>0),'Good Stability']=1\n",
    "        df.loc[(df['Good Stability'].isnull()),'Good Stability']=0\n",
    "        df.loc[(df['Bad Stability']>0),'Bad Stability']=1\n",
    "        df.loc[(df['Bad Stability'].isnull()),'Bad Stability']=0\n",
    "        df.loc[(df['Ads']>0),'Ads']=1\n",
    "        df.loc[(df['Ads'].isnull()),'Ads']=0\n",
    "        df.loc[(df['No Ads']>0),'No Ads']=1\n",
    "        df.loc[(df['No Ads'].isnull()),'No Ads']=0\n",
    "        # Labels are the values we want to predict\n",
    "        labels = np.array(df['App'])\n",
    "        # Remove the labels from the features\n",
    "        # axis 1 refers to the columns\n",
    "        features= df.drop('App', axis = 1)\n",
    "        # Saving feature names for later use\n",
    "        feature_list = list(features.columns)\n",
    "        # Convert to numpy array\n",
    "        features = np.array(features)\n",
    "        # Split the data into training and testing sets\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "        model_nb = MultinomialNB().fit(train_features, train_labels)\n",
    "        test_test = [[0,0,1,0,0,0,0,0]]\n",
    "        predict_nb_test = model_nb.predict(test_test)\n",
    "        if predict_nb_test == 1:\n",
    "            print('Spotify')\n",
    "        elif predict_nb_test == 2:\n",
    "            print('Pandora')\n",
    "        elif predict_nb_test == 3:\n",
    "            print('AppleMusic')\n",
    "        elif predict_nb_test == 4:\n",
    "            print('AmazonMusic')\n",
    "        with open(Task5().output().path, 'rb') as csv:\n",
    "            dbx.files_upload(csv.read(),'/DataScienceTeam9/merged_data.csv', mode=dropbox.files.WriteMode(\"overwrite\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    luigi.build([Task6()],local_scheduler=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
